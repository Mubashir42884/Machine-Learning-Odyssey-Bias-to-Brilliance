{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca8d5ef7",
   "metadata": {},
   "source": [
    "# 02. Gradient Descent\n",
    "### (Theory, Intuition, and Implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65acd87",
   "metadata": {},
   "source": [
    "In machine learning, our goal is often to find the best parameters (weights and biases) for a model that accurately predicts outcomes.  This involves **minimizing a \"cost function\"** which measures how poorly our model is performing. Gradient Descent is a powerful and widely used **iterative algorithm that helps us find the minimum of this cost function**.  It works by taking small, calculated steps downhill along the cost function's surface, guided by the gradient (slope) at each point.\n",
    "\n",
    "</br>\n",
    "\n",
    "### Why is this approach so effective?\n",
    "Gradient Descent is preferred because it's a relatively simple yet efficient way to **find the minimum of complex functions**, even when analytical solutions are impossible.  It's applicable in a wide range of machine learning models, including *linear regression, logistic regression, and neural networks*.\n",
    "\n",
    "### Where to use Gradient Descent? How does it helps?\n",
    "Specifically in **cost calculations**, gradient descent helps us find the parameters that **minimize the difference between our model's predictions and the actual values**, leading to a better fit.  Gradient Descent is typically employed when we have a **differentiable cost function and need to optimize model parameters**, making it a cornerstone of many machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4facfd3",
   "metadata": {},
   "source": [
    "#### Linear Regression Parameters & Functions\n",
    "* Linear Function, $ f_{w,b}(x) = {y} = wx + b $ , or also known as, Predicted Function, $ \\hat{y} = w_{pred}\\cdot x + b_{pred} $\n",
    "* Cost Function, $\\text{J(w,b)} = \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})^2 $\n",
    "</br>\n",
    "</br>\n",
    "\n",
    "\n",
    "#### Derivatives\n",
    "* Partial Derivative of Cost with $w$,  $$ J(w,b) \\text{ with } w =  \\frac{\\partial}{\\partial w} J(w,b) = \\frac{\\partial}{\\partial w} \\left[{\\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})^2} \\right] $$\n",
    "</br>\n",
    "$$ = \\frac{\\partial}{\\partial w} J(w,b) = \\frac{\\partial}{\\partial w} \\left[ \\frac{1}{2m} \\sum_{i=1}^{m} (f_{w,b}(x^{(i)}) - y^{(i)})^2 \\right] $$\n",
    "$$ = {2} \\cdot \\left[{ \\frac{1}{2m} \\sum_{i=1}^{m} [ {(f_{w,b}(x^{i})) - (w \\cdot x + b)}] } \\right] \\cdot (x^{i})$$\n",
    "$$ = \\frac{1}{m} \\sum_{i=1}^{m} \\left[ {(f_{w,b}(x^{i})) - (wx + b)} \\right] \\cdot (x^{i}) $$\n",
    "$$ \\color{dodgerblue}{= \\frac{1}{m} \\sum_{i=1}^{m} (f_{w,b}(x^{(i)}) - y^{(i)}) x^{(i)} ........^{(eq. 01)} } $$\n",
    "\n",
    "* Similarly, Partial Derivative of Cost with $b$,  $$ J(w,b) \\text{ with } b =  \\frac{\\partial}{\\partial b} J(w,b) = \\frac{\\partial}{\\partial b} [^{\\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})^2}] $$\n",
    "</br>\n",
    "$$ = \\frac{\\partial}{\\partial b} [^{ \\frac{1}{2m} \\sum_{i=1}^{m} [ {(f_{w,b}(x^{i})) - (w \\cdot x + b)}]^{2} }] $$\n",
    "$$ = {2} \\cdot [^{ \\frac{1}{2m} \\sum_{i=1}^{m} [ {(f_{w,b}(x^{i})) - (w \\cdot x + b)}] }]$$\n",
    "$$ = \\frac{1}{m} \\sum_{i=1}^{m} [ {(f_{w,b}(x^{i})) - (wx + b)}] $$\n",
    "$$ \\color{indigo}{= \\frac{1}{m} \\sum_{i=1}^{m} (f_{w,b}(x^{i}) - y^{(i)}) ........^{(eq. 02)} } $$\n",
    "\n",
    "</br>\n",
    "\n",
    "#### Gradient Descent Updating Method\n",
    "For each step towards training,</br>\n",
    "$ w_{new} = w_{old} - \\alpha \\color{dodgerblue}{\\frac{\\partial}{\\partial w} J(w,b)} = w_{old} - \\alpha \\cdot \\color{dodgerblue}{(eq. 01)}$\n",
    "\n",
    "$ b_{new} = b_{old} - \\alpha \\color{indigo}{\\frac{\\partial}{\\partial b} J(w,b)} = b_{old} - \\alpha \\cdot \\color{indigo}{(eq. 02)}$, where $ \\alpha $ is the learning rate.\n",
    "</br></br>\n",
    "\n",
    "#### Correct way of Implementing Gradient Descent Algorithm\n",
    "\n",
    "$\\textbf{function}$ $ \\textit{gradient_descent(ùõº, w, b)}:$\n",
    "\n",
    "   1. $temp_{w}$ $‚Üê w - ùõº \\cdot {\\frac{\\partial}{\\partial w} J(w,b)}$ `   # store new w`\n",
    "   2. $temp_{b}$ $‚Üê b - ùõº \\cdot {\\frac{\\partial}{\\partial b} J(w,b)}$ `   # store new b`\n",
    "   3. $w ‚Üê temp_{w}$ `   # update new w`\n",
    "   4. $b ‚Üê temp_{b}$ `   # update new b`\n",
    "\n",
    "\n",
    "**It uses temporary variables to ensure that the updates for w and b are calculated based on the same (original) values, effectively achieving a simultaneous update in a sequential programming environment. By not emphasizing the use of temporary variables, it could have led to an incorrect implementation where w is updated before b, violating the simultaneous update rule in Gradient Descent.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821bbeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40adf84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a linear function\n",
    "x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d008b734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53916e41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fbe506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b88470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2c3645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3acd0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847c3695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f4cab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599c0ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e87e22c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970de05b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae6846a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a064b67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9445bce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693dca13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c3fb78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863a058e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4cc7db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec290bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tf_GPU_Accl",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
